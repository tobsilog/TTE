{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgbXN_J5znuJ"
      },
      "source": [
        "INSTRUCTIONS\n",
        "\n",
        "Assignment 1 for Clustering:\n",
        "New and novel methods in Machine Learning are made either by borrowing formulas and concepts from other scientific fields and redefining it based on new sets of assumptions, or by adding an extra step to an already existing framework of methodology.\n",
        "\n",
        "In this exercise (Assignment 1 of the Clustering Topic), we will try to develop a novel method of Target Trial Emulation by integrating concepts of Clustering into the already existing framework. Target Trial Emulation is a new methodological framework in epidemiology which tries to account for the biases in old and traditional designs.\n",
        "\n",
        "These are the instructions:\n",
        "1. Look at this website: https://rpubs.com/alanyang0924/TTE\n",
        "2. Extract the dummy data in the package and save it as \"data_censored.csv\"\n",
        "2. Convert the R codes into Python Codes (use Jupyter Notebook), replicate the results using your python code.\n",
        "3. Create another copy of your Python Codes, name it TTE-v2 (use Jupyter Notebook).\n",
        "4. Using TTE-v2, think of a creative way on where you would integrate a clustering mechanism, understand each step carefully and decide at which step a clustering method can be implemented. Generate insights from your results.\n",
        "5. Do this by pair, preferably your thesis partner.\n",
        "6. Push to your github repository.\n",
        "7. Deadline is: March 9,  2025 at 11:59 pm.\n",
        "\n",
        "HINT: For those who dont have a thesis topic yet, you can actually develop a thesis topic out of this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels \n",
        "import linearmodels\n",
        "import sklearn\n",
        "import dowhy\n",
        "import causalml\n",
        "import econml\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import glm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from pathlib import Path\n",
        "from Functions.trial_seq import prepare_data_for_tte\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1:Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   id  period  treatment  x1        x2  x3        x4  age     age_s  outcome  \\\n",
            "0   1       0          1   1  1.146148   0  0.734203   36  0.083333        0   \n",
            "1   1       1          1   1  0.002200   0  0.734203   37  0.166667        0   \n",
            "2   1       2          1   0 -0.481762   0  0.734203   38  0.250000        0   \n",
            "3   1       3          1   0  0.007872   0  0.734203   39  0.333333        0   \n",
            "4   1       4          1   1  0.216054   0  0.734203   40  0.416667        0   \n",
            "\n",
            "   censored  eligible  \n",
            "0         0         1  \n",
            "1         0         0  \n",
            "2         0         0  \n",
            "3         0         0  \n",
            "4         0         0  \n"
          ]
        }
      ],
      "source": [
        "observational_data = pd.read_csv('dataset/data_censored.csv')\n",
        "pd.set_option('display.max_columns', None)\n",
        "print(observational_data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "# project_path = r\"D:\\Jean\\Documents\\TTE\"\n",
        "# # directory_name = \"trial_pp\"\n",
        "# directory_name = \"trial_itt\"\n",
        "\n",
        "# full_path = os.path.join(project_path, directory_name)\n",
        "\n",
        "# if not os.path.exists(full_path):\n",
        "#     os.mkdir(full_path)\n",
        "#     print(f\"Directory '{full_path}' created successfully.\")\n",
        "# else:\n",
        "#     print(f\"Directory '{full_path}' already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2:Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['id', 'period', 'treatment', 'x1', 'x2', 'x3', 'x4', 'age', 'age_s',\n",
            "       'outcome', 'censored', 'eligible'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "columns = observational_data.columns\n",
        "print(columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Per-Protocol Analysis Data:\n",
            "     id  period  treatment  x1        x2  x3        x4  age     age_s  \\\n",
            "0     1       0          1   1  1.146148   0  0.734203   36  0.083333   \n",
            "6     2       0          0   1 -0.802142   0 -0.990794   26 -0.750000   \n",
            "7     2       1          1   1 -0.983030   0 -0.990794   27 -0.666667   \n",
            "11    3       0          1   0  0.571029   1  0.391966   48  1.083333   \n",
            "19    4       0          0   0 -0.107079   1 -1.613258   29 -0.500000   \n",
            "..   ..     ...        ...  ..       ...  ..       ...  ...       ...   \n",
            "681  96       0          0   0 -1.954236   1 -1.293043   47  1.000000   \n",
            "682  96       1          1   0 -1.085325   1 -1.293043   48  1.083333   \n",
            "701  97       0          0   1  0.621108   1  0.830741   36  0.083333   \n",
            "702  98       0          1   1  1.392339   0  0.317418   64  2.416667   \n",
            "717  99       0          1   1 -0.346378   1  0.575268   65  2.500000   \n",
            "\n",
            "     outcome  censored  eligible  \n",
            "0          0         0         1  \n",
            "6          0         0         1  \n",
            "7          0         0         1  \n",
            "11         0         0         1  \n",
            "19         0         0         1  \n",
            "..       ...       ...       ...  \n",
            "681        0         0         1  \n",
            "682        0         0         1  \n",
            "701        0         1         1  \n",
            "702        0         0         1  \n",
            "717        0         0         1  \n",
            "\n",
            "[170 rows x 12 columns]\n",
            "\n",
            "Intention-to-Treat Analysis Data:\n",
            "     id  period  treatment  x1        x2  x3        x4  age     age_s  \\\n",
            "0     1       0          1   1  1.146148   0  0.734203   36  0.083333   \n",
            "1     1       1          1   1  0.002200   0  0.734203   37  0.166667   \n",
            "2     1       2          1   0 -0.481762   0  0.734203   38  0.250000   \n",
            "3     1       3          1   0  0.007872   0  0.734203   39  0.333333   \n",
            "4     1       4          1   1  0.216054   0  0.734203   40  0.416667   \n",
            "..   ..     ...        ...  ..       ...  ..       ...  ...       ...   \n",
            "720  99       3          0   0 -0.747906   1  0.575268   68  2.750000   \n",
            "721  99       4          0   0 -0.790056   1  0.575268   69  2.833333   \n",
            "722  99       5          1   1  0.387429   1  0.575268   70  2.916667   \n",
            "723  99       6          1   1 -0.033762   1  0.575268   71  3.000000   \n",
            "724  99       7          0   0 -1.340497   1  0.575268   72  3.083333   \n",
            "\n",
            "     outcome  censored  eligible  \n",
            "0          0         0         1  \n",
            "1          0         0         0  \n",
            "2          0         0         0  \n",
            "3          0         0         0  \n",
            "4          0         0         0  \n",
            "..       ...       ...       ...  \n",
            "720        0         0         0  \n",
            "721        0         0         0  \n",
            "722        0         0         0  \n",
            "723        0         0         0  \n",
            "724        1         0         0  \n",
            "\n",
            "[725 rows x 12 columns]\n"
          ]
        }
      ],
      "source": [
        "# Define a class to handle trial analysis\n",
        "class TrialAnalysis:\n",
        "    def __init__(self):\n",
        "        self.data = None\n",
        "        self.id = None\n",
        "        self.period = None\n",
        "        self.treatment = None\n",
        "        self.outcome = None\n",
        "        self.eligible = None\n",
        "\n",
        "    def set_data(self, data, id, period, treatment, outcome, eligible):\n",
        "        self.data = data\n",
        "        self.id = id\n",
        "        self.period = period\n",
        "        self.treatment = treatment\n",
        "        self.outcome = outcome\n",
        "        self.eligible = eligible\n",
        "        return self\n",
        "\n",
        "    def filter_eligible(self):\n",
        "        \"\"\"Filter data to include only eligible participants (for PP analysis).\"\"\"\n",
        "        if self.eligible is not None:\n",
        "            self.data = self.data[self.data[self.eligible] == True]\n",
        "        return self\n",
        "\n",
        "# Per-Protocol (PP) analysis\n",
        "trial_pp = TrialAnalysis().set_data(\n",
        "    data=observational_data,\n",
        "    id=\"id\",\n",
        "    period=\"period\",\n",
        "    treatment=\"treatment\",\n",
        "    outcome=\"outcome\",\n",
        "    eligible=\"eligible\"\n",
        ").filter_eligible()  # Filter to include only eligible participants\n",
        "\n",
        "# Intention-to-Treat (ITT) analysis\n",
        "trial_itt = TrialAnalysis().set_data(\n",
        "    data=observational_data,\n",
        "    id=\"id\",\n",
        "    period=\"period\",\n",
        "    treatment=\"treatment\",\n",
        "    outcome=\"outcome\",\n",
        "    eligible=\"eligible\"\n",
        ")  # No filtering for ITT\n",
        "\n",
        "# Check the results\n",
        "print(\"Per-Protocol Analysis Data:\")\n",
        "print(trial_pp.data)\n",
        "\n",
        "print(\"\\nIntention-to-Treat Analysis Data:\")\n",
        "print(trial_itt.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 3: Weight Models and censoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.1 Censoring due to treatment switching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.2 Other informative censoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PP dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ITT dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 4:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alright, let's break down what Target Trial Emulation (TTE) is, just like I'm explaining it to a college student who's brand new to data science. Think of this as building blocks, step by step.\n",
        "\n",
        "**Imagine you want to answer a really important question:**  Does a new study method actually *help* students learn better compared to the old method?\n",
        "\n",
        "To really know for sure, the **best way** to answer this question would be to run an **ideal experiment**.  Think of it like a perfectly designed science experiment in a lab, but with people and in the real world. This \"ideal experiment\" is what we call a **Target Trial**.\n",
        "\n",
        "**1. What is a \"Target Trial\"? (The Ideal Experiment in Our Heads)**\n",
        "\n",
        "Imagine the *perfect* way to figure out if the new study method is better.  This perfect way is our \"Target Trial\". Let's picture it:\n",
        "\n",
        "*   **Random Assignment:** We'd take a bunch of students and randomly assign half of them to use the **new study method** and the other half to use the **old study method**.  \"Random\" is key here – like flipping a coin for each student to decide which group they are in. This makes sure the groups are as similar as possible at the start, except for the study method they use.\n",
        "*   **Follow-up Over Time:** We'd then follow both groups of students for a certain amount of time, let's say a semester, and carefully track how well they are learning. We'd measure this maybe through exam scores, project grades, or participation.\n",
        "*   **Compare Outcomes:** At the end of the semester, we'd compare the learning outcomes (like average exam scores) between the two groups. If the group using the new study method consistently performs better than the group using the old method, we could be pretty confident that the new method is indeed more effective.\n",
        "\n",
        "**That, in a nutshell, is a Target Trial.** It's the gold standard, the ideal experiment we *wish* we could always run to answer questions about what works. It’s like having a perfect recipe for answering our question.\n",
        "\n",
        "**2.  The Problem:  We Can't Always Run the \"Perfect\" Trial in the Real World**\n",
        "\n",
        "In reality, running a perfect \"Target Trial\" is often **impossible** or **unethical**.  Think about it for our study method example:\n",
        "\n",
        "*   **Ethical Concerns:** Is it ethical to randomly assign students to a potentially *worse* study method just for the sake of an experiment?  Maybe the old method is known to be less effective, and we feel obligated to give everyone the best possible chance.\n",
        "*   **Practical Issues:**  It can be super hard to control everything in a real-world classroom or study environment. Students might use methods from both groups, they might drop out of the study, or other factors outside our control might influence their learning.\n",
        "*   **Time and Money:**  Running a large, well-controlled experiment takes a lot of time, money, and resources.\n",
        "\n",
        "**This is where \"Observational Data\" comes in, and this is where \"Target Trial Emulation\" becomes important.**\n",
        "\n",
        "**3.  Observational Data:  Real-World Information We Already Have**\n",
        "\n",
        "Instead of running a brand new experiment, we often have access to existing data that was collected in the real world, without a planned experiment. This is called **observational data**.\n",
        "\n",
        "Let's say a university has been using the old study method for years, and then some professors started trying out the new study method on their own. The university probably has data on student performance, which study methods were used (maybe not perfectly recorded!), student characteristics, etc.  This is observational data.\n",
        "\n",
        "**The Catch with Observational Data:**\n",
        "\n",
        "*   **No Random Assignment:**  In observational data, students weren't randomly assigned to study methods.  Professors and students chose which method to use, and these choices are often **not random**. For example, maybe the most motivated students choose to try the new method, or maybe professors who are already good teachers are more likely to adopt it.\n",
        "*   **Confusing Factors (Confounding):** Because of the lack of random assignment, it's hard to know if differences in outcomes are *really* because of the study method itself, or because of these other factors (like student motivation or teacher skill) that are mixed up with the choice of study method. These other factors are called **confounding factors**.\n",
        "\n",
        "**4. Target Trial Emulation: \"Playing Pretend\" with Observational Data**\n",
        "\n",
        "Target Trial Emulation (TTE) is like \"playing pretend\" with observational data.  Our goal is to use this messy, real-world data to **mimic** or **emulate** the \"Target Trial\" we described earlier as closely as possible.  We want to pretend we *did* run that perfect experiment, even though we didn't.\n",
        "\n",
        "**Think of it like this analogy:**\n",
        "\n",
        "Imagine you want to learn to bake a cake, but you don't have all the ingredients or a fancy oven.  Instead, you decide to \"emulate\" baking a cake: you might use a toy oven, pretend ingredients, and follow a simplified recipe. It's not the *real* cake, but by going through the steps, you learn about the process.\n",
        "\n",
        "TTE does something similar with data. We take our messy observational data and go through steps to *emulate* the key features of a Target Trial, especially **random assignment** (even though we didn't have it originally).\n",
        "\n",
        "**5.  Key Steps of Target Trial Emulation (Based on the Text you provided):**\n",
        "\n",
        "Let's break down the steps mentioned in your text to understand how we \"emulate\" a Target Trial:\n",
        "\n",
        "*   **a) Define the Estimand (What are we trying to measure?)**\n",
        "    *   The text mentions \"intention-to-treat (ITT)\" or \"per-protocol (PP)\".  These are fancy terms for *exactly* what question we are asking about the study methods.\n",
        "    *   **Intention-to-Treat (ITT):** We compare everyone who was *assigned* (or in our case, who we *emulate* as being assigned) to the new method group *versus* everyone assigned to the old method group, regardless of whether they actually stuck with that method throughout the whole time.  It's like asking: \"What is the effect of *assigning* the new method?\"\n",
        "    *   **Per-Protocol (PP):**  We only compare those who *perfectly followed* the new method *versus* those who *perfectly followed* the old method.  This is like asking: \"What is the effect of *actually using* the new method exactly as intended?\"\n",
        "    *   **Choosing the Estimand:** Deciding whether to focus on ITT or PP depends on the specific question you want to answer.  For example, ITT is often used because it's closer to the original random assignment idea and less affected by people dropping out or changing methods.\n",
        "\n",
        "*   **b) Prepare Observational Data:**\n",
        "    *   We need our observational data to have specific columns or information. The text mentions:\n",
        "        *   **Treatment:**  Which study method was used (new or old).\n",
        "        *   **Outcomes:**  How well students performed (exam scores, grades).\n",
        "        *   **Eligibility:**  Who was eligible to be included in our \"emulated\" trial.  We might want to set rules, like only including undergraduate students, or students in a specific department.\n",
        "\n",
        "*   **c) Censoring and IPCW (Dealing with People Dropping Out or Changing Methods):**\n",
        "    *   **Censoring:** In a real study (or our emulated one), things can go wrong. Students might:\n",
        "        *   **Treatment Switching:** Start with the new method but then switch to the old one, or vice-versa.\n",
        "        *   **Informative Censoring:** Drop out of the study altogether, and the reason they drop out might be related to the study method or their learning progress.  This is called \"informative\" because the dropout tells us something important.\n",
        "    *   **Inverse Probability of Censoring Weights (IPCW):**  To deal with these problems, especially \"informative censoring,\" we use a clever technique called IPCW. Think of it like giving more \"weight\" to the data from students who are similar to those who dropped out, but *didn't* drop out.  It's a way to statistically adjust for the bias caused by people leaving the study in a non-random way.  The text mentions \"separate models\" to calculate these weights, which is a bit more technical but just know it's about making adjustments.\n",
        "\n",
        "*   **d) Expand the Observational Dataset into Trials:**\n",
        "    *   This is a bit more advanced and depends on the specifics of the `TrialEmulation` package in R. But the general idea is to take our observational data, which might be collected over a long period, and break it down into a sequence of \"mini-trials.\"\n",
        "    *   Imagine we have student data over several semesters. We might \"expand\" this data to create a series of \"trials,\" maybe one trial for each semester. This allows us to analyze how the study methods perform over time and in different groups.\n",
        "    *   The \"expansion options\" mentioned in the text are ways to control *how* we break down the data into these mini-trials.\n",
        "\n",
        "*   **e) Fit a Marginal Structural Model (MSM):**\n",
        "    *   MSM is a statistical technique used to analyze the \"expanded\" data and estimate the causal effects.  Think of it as a sophisticated tool to untangle the relationship between study methods and student outcomes, while accounting for all the potential confounding factors and biases we've tried to address.\n",
        "    *   It's called \"marginal\" because it looks at the average effect of the treatment across the whole population, rather than focusing on specific subgroups.\n",
        "\n",
        "*   **f) Predictions and Visualization:**\n",
        "    *   After fitting the MSM, we can make predictions.  For example, we can predict the \"survival probabilities\" – in our case, maybe the probability of students achieving a certain grade level over time, for both the new and old study methods.  Or we could look at \"cumulative incidences\" – like the percentage of students who fail a course in each group.\n",
        "    *   **Visualization:** The results are then visualized using graphs, charts, etc., to show the differences in outcomes between the study methods over time.  This makes it easier to see if there's a meaningful difference and to understand the size and direction of the effect.\n",
        "\n",
        "**6. TrialEmulation Package in R:**\n",
        "\n",
        "The text mentions that this whole process is implemented in R using the `TrialEmulation` package.  R is a programming language widely used in data science and statistics. This package provides tools and functions to help researchers carry out all the steps of TTE in a systematic way, from data preparation to model fitting and visualization.\n",
        "\n",
        "**In Summary: Why is TTE Useful?**\n",
        "\n",
        "Target Trial Emulation is a powerful approach because:\n",
        "\n",
        "*   **It tries to get as close as possible to the \"gold standard\" (randomized controlled trial) when we can't actually run one.**\n",
        "*   **It helps us use real-world, observational data to answer important causal questions more reliably.**\n",
        "*   **It forces us to be very clear about what our \"ideal\" experiment would look like and what assumptions we are making when we use observational data.**\n",
        "*   **It provides a structured framework for addressing common problems in observational data, like confounding and censoring.**\n",
        "\n",
        "**Think of it this way:**  If you can't bake a real cake, emulating the process is the next best thing to learn about baking and maybe even get a pretty good (though not perfect) idea of what the cake would be like!  Similarly, TTE helps us learn about cause and effect from observational data, even when we can't run the perfect experiment.\n",
        "\n",
        "This is a high-level overview.  If you go into data science, you'll learn much more about each of these steps in detail. But hopefully, this gives you a good starting understanding of what Target Trial Emulation is all about! Let me know if any part is unclear, and I can try to explain it in a different way.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
